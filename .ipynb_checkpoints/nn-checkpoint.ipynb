{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "282c2085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'raise', 'over': 'raise', 'under': 'raise', 'invalid': 'raise'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from sklearn.utils import shuffle\n",
    "np.seterr(all='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5d292d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing MNIST\n",
      "Preprocessing finished\n"
     ]
    }
   ],
   "source": [
    "dataset_size = 5000\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "train_X = train_X[:5000]\n",
    "train_y = train_y[:5000]\n",
    "print(\"Preprocessing MNIST\")\n",
    "prep_train_x = []\n",
    "for i in range(len(train_X)):\n",
    "    prep_train_x.append((np.array(train_X[i])/255.0))\n",
    "prep_train_x = np.array(prep_train_x)\n",
    "train_X = prep_train_x\n",
    "print(\"Preprocessing finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f4a99ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, train_data, hidden_layers_sizes=[4, 3], output_layer_size=2,\n",
    "                 epochs=1, batch_size=1, learning_rate=1):\n",
    "        self.debug = False\n",
    "        self.train_input = train_data[0][0]\n",
    "        self.train_output = train_data[0][1]\n",
    "        print(self.train_input)\n",
    "        print(self.train_output)\n",
    "        self.train_data = train_data\n",
    "#         self.input_layer_size = np.prod(np.shape(self.train_input[0]))\n",
    "        self.input_layer_size = np.prod(np.shape(self.train_input))\n",
    "        self.hidden_layers_sizes = hidden_layers_sizes\n",
    "        self.output_layer_size = output_layer_size\n",
    "        self.layers = []\n",
    "        self.layers_inputs = []\n",
    "        self.weighs = []\n",
    "        self.biases = []\n",
    "        self.overall_error = []\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.init_layers()\n",
    "        self.init_weights_and_biases()\n",
    "        \n",
    "    def init_layers(self):\n",
    "        \"\"\"\n",
    "        self.layers is a list of all layers that nn consists of\n",
    "        \"\"\"\n",
    "        hls = ([None] * size for size in self.hidden_layers_sizes)\n",
    "        self.layers.append([None] * self.input_layer_size)\n",
    "        [self.layers.append(hl) for hl in hls]\n",
    "        self.layers.append([None] * self.output_layer_size)\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[i] = np.transpose(np.array(self.layers[i]))\n",
    "        self.layers_inputs = self.layers[1:]\n",
    "\n",
    "    def init_weights_and_biases(self):\n",
    "        \"\"\"\n",
    "        Initialisation of nn weights matrices and biases vectors\n",
    "        \"\"\"\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.weighs.append(np.random.random((len(self.layers[i + 1]), len(self.layers[i]))))\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.biases.append(np.random.random((len(self.layers[i]))))\n",
    "        # if self.debug:\n",
    "        for weigh in self.weighs:\n",
    "            print(np.shape(weigh))\n",
    "\n",
    "\n",
    "#     def init_layers(self):\n",
    "#         \"\"\"\n",
    "#         self.layers is a list of all layers that nn consists of\n",
    "#         \"\"\"\n",
    "#         hls = ([None] * size for size in self.hidden_layers_sizes)\n",
    "#         self.layers.append([None] * self.input_layer_size)\n",
    "#         [self.layers.append(hl) for hl in hls]\n",
    "#         self.layers.append([None] * self.output_layer_size)\n",
    "#         for i in range(len(self.layers)):\n",
    "#             self.layers[i] = np.transpose(np.array(self.layers[i]))\n",
    "#         self.layers_inputs = self.layers[1:]\n",
    "\n",
    "#     def init_weights_and_biases(self):\n",
    "#         \"\"\"\n",
    "#         Initialisation of nn weights matrices and biases vectors\n",
    "#         \"\"\"\n",
    "# #         for i in range(len(self.layers) - 1):\n",
    "# #             self.weighs.append(np.random.uniform(low=-0.3, high=0.3, size=(len(self.layers[i + 1]), len(self.layers[i]))))\n",
    "# #         for i in range(1, len(self.layers)):\n",
    "# #             self.biases.append(np.random.uniform(low=-0.3, high=0.3, size=(len(self.layers[i]))))\n",
    "# #         if self.debug:\n",
    "# #             for weigh in self.weighs:\n",
    "# #                 print(np.shape(weigh))\n",
    "#         self.weighs.append(np.transpose(np.array([np.array([0.3, 0.2, 0.4, 0.5]), np.array([0.7, 0.6, 0.2, 0.5])])))\n",
    "#         self.weighs.append(np.transpose(np.array([np.array([0.2, 0.8, 0.9]), np.array([0.3, 0.4, 0.3]), \n",
    "#                                      np.array([0.5, 0.1, 0.2]), np.array([0.8, 0.2, 0.2])])))\n",
    "#         self.weighs.append(np.transpose(np.array([np.array([0.6, 0.5]), np.array([0.2, 0.3]), np.array([0.1, 0.8])])))\n",
    "        \n",
    "#         self.biases.append(np.array([0.1, 0.2, 0.1, 0.2]))\n",
    "#         self.biases.append(np.array([0.3, 0.4, 0.6]))\n",
    "#         self.biases.append(np.array([0.2, 0.5]))\n",
    "#         print(self.weighs)\n",
    "    \n",
    "    def sigmoid(self, v):\n",
    "        # Sigmoid function flattens very fast. Raising to power v which can be very high\n",
    "        # is extremely computationally demanding and slows down the training\n",
    "        sigmoid_v = []\n",
    "        for e in v:\n",
    "            if e > 50:\n",
    "                e = 50\n",
    "            if e < -50:\n",
    "                e = -50\n",
    "            sigmoid_v.append(1.0 / (1.0 + np.exp(-e)))\n",
    "        return np.array(sigmoid_v)\n",
    "\n",
    "    def sigmoid_der(self, v):\n",
    "        # Sigmoid function flattens very fast. Raising to power v which can be very high\n",
    "        # is computationally demanding and slows down the training\n",
    "        sigmoid_der_v = []\n",
    "        for e in v:\n",
    "            if e > 50:\n",
    "                e = 50\n",
    "            if e < -50:\n",
    "                e = -50\n",
    "            sigmoid_der_v.append(np.exp(-e) / ((np.exp(-e) + 1) ** 2))\n",
    "        return np.array(sigmoid_der_v)\n",
    "\n",
    "    def feedforward(self, input_layer):\n",
    "        # Transposing input for matrix multiplication\n",
    "        current_layer = np.transpose(np.array(input_layer))\n",
    "        # Assigning activations in first layer to input values\n",
    "        self.layers[0] = current_layer\n",
    "        # Calculating output vector using matrix multiplication\n",
    "        for i in range(len(self.weighs)):\n",
    "            print(\"WEIGHS\")\n",
    "            print(self.weighs[i])\n",
    "            print(\"LAYER\")\n",
    "            print(current_layer)\n",
    "            print(\"MATMUL\")\n",
    "            print(np.matmul(self.weighs[i], current_layer))\n",
    "            # Saving vectors of inputs to consecutive net layers, that will later\n",
    "            # be used during the error backpropagation algorithm\n",
    "            print(\"INFEEDFORWARD\")\n",
    "            print(self.layers)\n",
    "            self.layers_inputs[i] = np.add(np.matmul(self.weighs[i], current_layer), self.biases[i])\n",
    "            # Saving vectors of activations of consecutive net layers, that will later\n",
    "            # be used during the error backpropagation algorithm\n",
    "            # index is i+1 since there is 1 more layer than weight vectors (first layer)\n",
    "            # so we update all layers instead of first. First was assigned to input layer\n",
    "            # before the loop          \n",
    "            self.layers[i + 1] = self.sigmoid(np.add(np.matmul(self.weighs[i], current_layer), self.biases[i]))\n",
    "#             if i == 0:\n",
    "#                 print(\"IN FEEDFORWARD\")\n",
    "#                 print(\"FIRST LAYER\")\n",
    "#                 print(self.layers[i])\n",
    "#                 print(\"SECOND LAYER\")\n",
    "#                 print(self.layers[i + 1])\n",
    "            current_layer = self.sigmoid(np.add(np.matmul(self.weighs[i], current_layer), self.biases[i]))\n",
    "#         print(\"LAST LAYER IN FEEDFORWARD\")\n",
    "#         print(current_layer)\n",
    "        return current_layer\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.epochs):\n",
    "            epoch_error = self.train_epoch()\n",
    "            print(f'{\"Epoch: \"}{i}{\" / \"}{self.epochs}')\n",
    "            print(f'{\"Error: \"}{epoch_error}')\n",
    "\n",
    "#     def train_epoch(self):\n",
    "#         epoch_error = 0\n",
    "# #         batches = self.create_batches()\n",
    "#         batches = self.train_data\n",
    "#         print(\"BATCHES\")\n",
    "#         print(batches)\n",
    "# #         for batch in batches:\n",
    "# #             print(\"BATCH\")\n",
    "# #             print(batch)\n",
    "#         batch = batches[0]\n",
    "#         epoch_error += self.train_batch(batch)\n",
    "#         # Adding epoch cost to the list containing all losses across all epochs\n",
    "#         self.overall_error.append(epoch_error)\n",
    "#         return epoch_error\n",
    "\n",
    "#     def train_batch(self, batch):\n",
    "#         inputs = batch[0]\n",
    "#         outputs = batch[1]\n",
    "#         print(\"inputs\")\n",
    "#         print(inputs)\n",
    "#         print(\"outputs\")\n",
    "#         print(outputs)\n",
    "#         batch_output_error = 0\n",
    "#         batch_error_vectors = [np.zeros(len(layer)) for layer in self.layers][1:]\n",
    "# #         for k in range(len(inputs)):\n",
    "#             # Inputs in batch\n",
    "#         i = inputs\n",
    "#         # Outputs in batch\n",
    "#         o = outputs\n",
    "#         feedforward_output = self.feedforward(i)\n",
    "# #         net_output = self.net_output(o)\n",
    "        \n",
    "# #         batch_output_error += self.example_error(feedforward_output, net_output)\n",
    "#         example_error = self.example_error(feedforward_output, o)\n",
    "#         batch_output_error += example_error\n",
    "#         # Running backpropagation algorithm to calculate errors on\n",
    "#         # weights and biases on every layer\n",
    "# #         example_error_vectors = self.backprop(self.example_error(feedforward_output, net_output))\n",
    "#         example_error_vectors = self.backprop(example_error)\n",
    "#         # Updating overall batch error with example error values\n",
    "#         for i in range(len(batch_error_vectors)):\n",
    "# #                 if i == len(batch_error_vectors) - 1:\n",
    "# #                     print(\"ERROR ON LAST LAYER\")\n",
    "# #                     print(example_error_vectors[len(batch_error_vectors) - i - 1])\n",
    "#             batch_error_vectors[i] += example_error_vectors[len(batch_error_vectors) - i - 1]\n",
    "    \n",
    "# #         for k in range(len(inputs)):\n",
    "# #             # Inputs in batch\n",
    "# #             i = self.flatten_matrix(inputs[k])\n",
    "# #             # Outputs in batch\n",
    "# #             o = self.flatten_matrix(outputs[k])\n",
    "# #             batch_output_error += self.example_error(self.feedforward(i), self.net_output(o))\n",
    "# #             # Running backpropagation algorithm to calculate errors on\n",
    "# #             # weights and biases on every layer\n",
    "# #             example_error_vectors = self.backprop(self.example_error(self.feedforward(i), self.net_output(o)))\n",
    "# #             # Updating overall batch error with example error values\n",
    "# #             for i in range(len(batch_error_vectors)):\n",
    "# # #                 if i == len(batch_error_vectors) - 1:\n",
    "# # #                     print(\"ERROR ON LAST LAYER\")\n",
    "# # #                     print(example_error_vectors[len(batch_error_vectors) - i - 1])\n",
    "# #                 batch_error_vectors[i] += example_error_vectors[len(batch_error_vectors) - i - 1]\n",
    "#         # After all batch layers errors are calculated, SGD is performed\n",
    "#         # to update weights and biases\n",
    "#         self.sgd(batch_error_vectors)\n",
    "#         return batch_output_error\n",
    "\n",
    "    def train_epoch(self):\n",
    "        epoch_error = 0\n",
    "        batches = self.create_batches()\n",
    "        for batch in batches:\n",
    "            epoch_error += self.train_batch(batch)\n",
    "        # Adding epoch cost to the list containing all losses across all epochs\n",
    "        self.overall_error.append(epoch_error)\n",
    "        return epoch_error\n",
    "\n",
    "    def train_batch(self, batch):\n",
    "        inputs = batch[0]\n",
    "        outputs = batch[1]\n",
    "        batch_output_error = 0\n",
    "        batch_error_vectors = [np.zeros(len(layer)) for layer in self.layers][1:]\n",
    "        for k in range(len(inputs)):\n",
    "            # Inputs in batch\n",
    "            i = self.flatten_matrix(inputs[k])\n",
    "            # Outputs in batch\n",
    "            o = self.flatten_matrix(outputs[k])\n",
    "            batch_output_error += self.example_error(self.feedforward(i), self.net_output(o))\n",
    "            # Running backpropagation algorithm to calculate errors on\n",
    "            # weights and biases on every layer\n",
    "            example_error_vectors = self.backprop(self.example_error(self.feedforward(i), self.net_output(o)))\n",
    "            # Updating overall batch error with example error values\n",
    "            for i in range(len(batch_error_vectors)):\n",
    "                batch_error_vectors[i] += example_error_vectors[len(batch_error_vectors) - i - 1]\n",
    "        # After all batch layers errors are calculated, SGD is performed\n",
    "        # to update weights and biases\n",
    "        self.sgd(batch_error_vectors)\n",
    "        return batch_output_error\n",
    "\n",
    "    def backprop(self, output_error_vector):\n",
    "        example_error_vectors = []\n",
    "        current_error_vector = output_error_vector\n",
    "        # Appending the vector of errors in output layer\n",
    "        # to the example_error_vectors\n",
    "        example_error_vectors.append(current_error_vector)\n",
    "        for i in range(len(self.layers) - 1, 1, -1):\n",
    "            current_error_vector = np.multiply(np.matmul(np.transpose(self.weighs[i - 1]), current_error_vector),\n",
    "                                               np.array(self.sigmoid_der(self.layers_inputs[i - 2])))\n",
    "#             if i == len(self.layers) - 1:\n",
    "#                 print(\"PRE-LAST LAYER\")\n",
    "#                 print(print(current_error_vector))\n",
    "            example_error_vectors.append(current_error_vector)\n",
    "        return example_error_vectors\n",
    "\n",
    "    def sgd(self, batch_error_vectors):\n",
    "        print(\"BEV\")\n",
    "        print(batch_error_vectors)\n",
    "        for i in range(len(self.weighs)):\n",
    "            # Update the values in the vector of weights in the steepest gradient direction\n",
    "            print(\"DIFFERENCE\")\n",
    "            print(np.multiply(self.learning_rate / self.batch_size, np.sum(\n",
    "                np.matmul(batch_error_vectors[i], np.transpose(self.layers[i + 1])))))\n",
    "            \n",
    "            self.weighs[i] = np.subtract(self.weighs[i], np.multiply(self.learning_rate / self.batch_size, np.sum(\n",
    "                np.matmul(batch_error_vectors[i], np.transpose(self.layers[i + 1])))))\n",
    "            # Update the values in the vector of biases in the steepest gradient direction\n",
    "            self.biases[i] = np.subtract(self.biases[i], np.multiply(self.learning_rate / self.batch_size, np.sum(\n",
    "                batch_error_vectors[i])))\n",
    "            \n",
    "#             print(\"LAYER\")\n",
    "#             print(i)\n",
    "#             print(\"NP SUM OD BATCH ERROR VECTORS\")\n",
    "#             print(np.sum(batch_error_vectors[i]))\n",
    "\n",
    "    def example_error(self, net_output, expected_output):\n",
    "        try:\n",
    "            print(\"NET OUTPUT\")\n",
    "            print(net_output)\n",
    "            print(\"EXPECTED OUTPUT\")\n",
    "            print(expected_output)\n",
    "            print(\"ERROR\")\n",
    "            print(np.multiply(np.divide(np.square(np.subtract(net_output, expected_output)), 2), \n",
    "                  self.sigmoid_der(self.layers_inputs[-1])))\n",
    "#             return np.divide(np.square(np.subtract(net_output, expected_output)), 2)\n",
    "            return np.multiply(np.divide(np.square(np.subtract(net_output, expected_output)), 2), \n",
    "                  self.sigmoid_der(self.layers_inputs[-1]))\n",
    "        except:\n",
    "            raise\n",
    "\n",
    "    def create_batches(self):\n",
    "        batches = []\n",
    "        # Shuffling training dataset\n",
    "        sh_t_in, sh_t_out = shuffle(self.train_input, self.train_output)\n",
    "        # Sampling a mini-batch from the dataset\n",
    "        i = 0\n",
    "        while i < len(sh_t_in) - self.batch_size:\n",
    "            batches.append((sh_t_in[i:i + self.batch_size], sh_t_out[i:i + self.batch_size]))\n",
    "            i += self.batch_size\n",
    "        return batches\n",
    "\n",
    "    def net_output(self, output):\n",
    "        net_output = np.zeros(10)\n",
    "        net_output[output] = 1\n",
    "        return net_output\n",
    "\n",
    "    def flatten_matrix(self, matrix):\n",
    "        if len(np.shape(matrix)) > 1:\n",
    "            return matrix.flatten()\n",
    "        else:\n",
    "            return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "245a6708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5 0.2]\n",
      "[1. 0.]\n",
      "[array([[0.3, 0.7],\n",
      "       [0.2, 0.6],\n",
      "       [0.4, 0.2],\n",
      "       [0.5, 0.5]]), array([[0.2, 0.3, 0.5, 0.8],\n",
      "       [0.8, 0.4, 0.1, 0.2],\n",
      "       [0.9, 0.3, 0.2, 0.2]]), array([[0.6, 0.2, 0.1],\n",
      "       [0.5, 0.3, 0.8]])]\n",
      "BATCHES\n",
      "(array([[0.5, 0.2],\n",
      "       [1. , 0. ]]), array([], dtype=float64))\n",
      "inputs\n",
      "[0.5 0.2]\n",
      "outputs\n",
      "[1. 0.]\n",
      "WEIGHS\n",
      "[[0.3 0.7]\n",
      " [0.2 0.6]\n",
      " [0.4 0.2]\n",
      " [0.5 0.5]]\n",
      "LAYER\n",
      "[0.5 0.2]\n",
      "MATMUL\n",
      "[0.29 0.22 0.24 0.35]\n",
      "INFEEDFORWARD\n",
      "[array([0.5, 0.2]), array([None, None, None, None], dtype=object), array([None, None, None], dtype=object), array([None, None], dtype=object)]\n",
      "WEIGHS\n",
      "[[0.2 0.3 0.5 0.8]\n",
      " [0.8 0.4 0.1 0.2]\n",
      " [0.9 0.3 0.2 0.2]]\n",
      "LAYER\n",
      "[0.5962827  0.60348325 0.58419052 0.63413559]\n",
      "MATMUL\n",
      "[1.09970525 0.90366563 0.96136463]\n",
      "INFEEDFORWARD\n",
      "[array([0.5, 0.2]), array([0.5962827 , 0.60348325, 0.58419052, 0.63413559]), array([None, None, None], dtype=object), array([None, None], dtype=object)]\n",
      "WEIGHS\n",
      "[[0.6 0.2 0.1]\n",
      " [0.5 0.3 0.8]]\n",
      "LAYER\n",
      "[0.80213711 0.78645126 0.82654908]\n",
      "MATMUL\n",
      "[0.72122743 1.2982432 ]\n",
      "INFEEDFORWARD\n",
      "[array([0.5, 0.2]), array([0.5962827 , 0.60348325, 0.58419052, 0.63413559]), array([0.80213711, 0.78645126, 0.82654908]), array([None, None], dtype=object)]\n",
      "NET OUTPUT\n",
      "[0.71529214 0.85793495]\n",
      "EXPECTED OUTPUT\n",
      "[1. 0.]\n",
      "ERROR\n",
      "[0.00825376 0.04485598]\n",
      "BEV\n",
      "[array([0.00183811, 0.00093263, 0.00084512, 0.00116851]), array([0.00434561, 0.00253725, 0.00526298]), array([0.00825376, 0.04485598])]\n",
      "DIFFERENCE\n",
      "0.002893570722094902\n",
      "DIFFERENCE\n",
      "0.009831301785119467\n",
      "DIFFERENCE\n",
      "0.04438736160640011\n",
      "Epoch: 0 / 1\n",
      "Error: [0.00825376 0.04485598]\n"
     ]
    }
   ],
   "source": [
    "test_train_X = np.array([np.array([0.5, 0.2]), np.array([1, 0])])\n",
    "test_train_y = np.array([])\n",
    "nn = NeuralNet((test_train_X, test_train_y))\n",
    "# batches = nn.create_batches()\n",
    "batches = nn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c68eef03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8427dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "640d35bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.01176471 0.07058824 0.07058824 0.07058824 0.49411765 0.53333333\n",
      "  0.68627451 0.10196078 0.65098039 1.         0.96862745 0.49803922\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
      "  0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      "  0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.19215686 0.93333333 0.99215686 0.99215686 0.99215686\n",
      "  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.98431373\n",
      "  0.36470588 0.32156863 0.32156863 0.21960784 0.15294118 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.07058824 0.85882353 0.99215686 0.99215686 0.99215686\n",
      "  0.99215686 0.99215686 0.77647059 0.71372549 0.96862745 0.94509804\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
      "  0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.05490196 0.00392157 0.60392157\n",
      "  0.99215686 0.35294118 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.54509804\n",
      "  0.99215686 0.74509804 0.00784314 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.04313725\n",
      "  0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.1372549  0.94509804 0.88235294 0.62745098 0.42352941 0.00392157\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.31764706 0.94117647 0.99215686 0.99215686 0.46666667\n",
      "  0.09803922 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
      "  0.58823529 0.10588235 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.0627451  0.36470588 0.98823529\n",
      "  0.99215686 0.73333333 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.97647059\n",
      "  0.99215686 0.97647059 0.25098039 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
      "  0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.15294118 0.58039216 0.89803922 0.99215686 0.99215686 0.99215686\n",
      "  0.98039216 0.71372549 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.09411765 0.44705882\n",
      "  0.86666667 0.99215686 0.99215686 0.99215686 0.99215686 0.78823529\n",
      "  0.30588235 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
      "  0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.07058824 0.67058824 0.85882353 0.99215686 0.99215686 0.99215686\n",
      "  0.99215686 0.76470588 0.31372549 0.03529412 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.21568627 0.6745098\n",
      "  0.88627451 0.99215686 0.99215686 0.99215686 0.99215686 0.95686275\n",
      "  0.52156863 0.04313725 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.53333333 0.99215686\n",
      "  0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.2        0.62352941 0.99215686\n",
      "  0.62352941 0.19607843 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.18823529 0.93333333 0.98823529 0.98823529\n",
      "  0.98823529 0.92941176 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.21176471 0.89019608 0.99215686 0.98823529 0.9372549\n",
      "  0.91372549 0.98823529 0.22352941 0.02352941 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.03921569\n",
      "  0.23529412 0.87843137 0.98823529 0.99215686 0.98823529 0.79215686\n",
      "  0.32941176 0.98823529 0.99215686 0.47843137 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.63921569\n",
      "  0.98823529 0.98823529 0.98823529 0.99215686 0.98823529 0.98823529\n",
      "  0.37647059 0.74117647 0.99215686 0.65490196 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2        0.93333333\n",
      "  0.99215686 0.99215686 0.74509804 0.44705882 0.99215686 0.89411765\n",
      "  0.18431373 0.30980392 1.         0.65882353 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.18823529 0.93333333 0.98823529\n",
      "  0.98823529 0.70196078 0.04705882 0.29411765 0.4745098  0.08235294\n",
      "  0.         0.         0.99215686 0.95294118 0.19607843 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.14901961 0.64705882 0.99215686 0.91372549\n",
      "  0.81568627 0.32941176 0.         0.         0.         0.\n",
      "  0.         0.         0.99215686 0.98823529 0.64705882 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.02745098 0.69803922 0.98823529 0.94117647 0.27843137\n",
      "  0.0745098  0.10980392 0.         0.         0.         0.\n",
      "  0.         0.         0.99215686 0.98823529 0.76470588 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.22352941 0.98823529 0.98823529 0.24705882 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.99215686 0.98823529 0.76470588 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.77647059 0.99215686 0.74509804 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.         0.99215686 0.76862745 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.29803922 0.96470588 0.98823529 0.43921569 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.99215686 0.98823529 0.58039216 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.98823529 0.90196078 0.09803922 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.02745098 0.52941176 0.99215686 0.72941176 0.04705882 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.98823529 0.8745098  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.02745098\n",
      "  0.51372549 0.98823529 0.88235294 0.27843137 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.98823529 0.56862745 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.18823529 0.64705882\n",
      "  0.98823529 0.67843137 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.3372549  0.99215686 0.88235294 0.         0.         0.\n",
      "  0.         0.         0.         0.44705882 0.93333333 0.99215686\n",
      "  0.63529412 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.98823529 0.97647059 0.57254902 0.18823529 0.11372549\n",
      "  0.33333333 0.69803922 0.88235294 0.99215686 0.8745098  0.65490196\n",
      "  0.21960784 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.98823529 0.98823529 0.98823529 0.89803922 0.84313725\n",
      "  0.98823529 0.98823529 0.98823529 0.76862745 0.50980392 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.10980392 0.78039216 0.98823529 0.98823529 0.99215686 0.98823529\n",
      "  0.98823529 0.91372549 0.56862745 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.09803922 0.50196078 0.98823529 0.99215686 0.98823529\n",
      "  0.55294118 0.14509804 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n",
      "[array([[0.3, 0.7],\n",
      "       [0.2, 0.6],\n",
      "       [0.4, 0.2],\n",
      "       [0.5, 0.5]]), array([[0.2, 0.3, 0.5, 0.8],\n",
      "       [0.8, 0.4, 0.1, 0.2],\n",
      "       [0.9, 0.3, 0.2, 0.2]]), array([[0.6, 0.2, 0.1],\n",
      "       [0.5, 0.3, 0.8]])]\n",
      "BATCHES\n",
      "(array([[[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]]]), array([5, 0, 4, ..., 2, 1, 2], dtype=uint8))\n",
      "inputs\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.01176471 0.07058824 0.07058824 0.07058824 0.49411765 0.53333333\n",
      "  0.68627451 0.10196078 0.65098039 1.         0.96862745 0.49803922\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
      "  0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      "  0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.19215686 0.93333333 0.99215686 0.99215686 0.99215686\n",
      "  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.98431373\n",
      "  0.36470588 0.32156863 0.32156863 0.21960784 0.15294118 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.07058824 0.85882353 0.99215686 0.99215686 0.99215686\n",
      "  0.99215686 0.99215686 0.77647059 0.71372549 0.96862745 0.94509804\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
      "  0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.05490196 0.00392157 0.60392157\n",
      "  0.99215686 0.35294118 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.54509804\n",
      "  0.99215686 0.74509804 0.00784314 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.04313725\n",
      "  0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.1372549  0.94509804 0.88235294 0.62745098 0.42352941 0.00392157\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.31764706 0.94117647 0.99215686 0.99215686 0.46666667\n",
      "  0.09803922 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
      "  0.58823529 0.10588235 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.0627451  0.36470588 0.98823529\n",
      "  0.99215686 0.73333333 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.97647059\n",
      "  0.99215686 0.97647059 0.25098039 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
      "  0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.15294118 0.58039216 0.89803922 0.99215686 0.99215686 0.99215686\n",
      "  0.98039216 0.71372549 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.09411765 0.44705882\n",
      "  0.86666667 0.99215686 0.99215686 0.99215686 0.99215686 0.78823529\n",
      "  0.30588235 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
      "  0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.07058824 0.67058824 0.85882353 0.99215686 0.99215686 0.99215686\n",
      "  0.99215686 0.76470588 0.31372549 0.03529412 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.21568627 0.6745098\n",
      "  0.88627451 0.99215686 0.99215686 0.99215686 0.99215686 0.95686275\n",
      "  0.52156863 0.04313725 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.53333333 0.99215686\n",
      "  0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n",
      "outputs\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.2        0.62352941 0.99215686\n",
      "  0.62352941 0.19607843 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.18823529 0.93333333 0.98823529 0.98823529\n",
      "  0.98823529 0.92941176 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.21176471 0.89019608 0.99215686 0.98823529 0.9372549\n",
      "  0.91372549 0.98823529 0.22352941 0.02352941 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.03921569\n",
      "  0.23529412 0.87843137 0.98823529 0.99215686 0.98823529 0.79215686\n",
      "  0.32941176 0.98823529 0.99215686 0.47843137 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.63921569\n",
      "  0.98823529 0.98823529 0.98823529 0.99215686 0.98823529 0.98823529\n",
      "  0.37647059 0.74117647 0.99215686 0.65490196 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2        0.93333333\n",
      "  0.99215686 0.99215686 0.74509804 0.44705882 0.99215686 0.89411765\n",
      "  0.18431373 0.30980392 1.         0.65882353 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.18823529 0.93333333 0.98823529\n",
      "  0.98823529 0.70196078 0.04705882 0.29411765 0.4745098  0.08235294\n",
      "  0.         0.         0.99215686 0.95294118 0.19607843 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.14901961 0.64705882 0.99215686 0.91372549\n",
      "  0.81568627 0.32941176 0.         0.         0.         0.\n",
      "  0.         0.         0.99215686 0.98823529 0.64705882 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.02745098 0.69803922 0.98823529 0.94117647 0.27843137\n",
      "  0.0745098  0.10980392 0.         0.         0.         0.\n",
      "  0.         0.         0.99215686 0.98823529 0.76470588 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.22352941 0.98823529 0.98823529 0.24705882 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.99215686 0.98823529 0.76470588 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.77647059 0.99215686 0.74509804 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.         0.99215686 0.76862745 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.29803922 0.96470588 0.98823529 0.43921569 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.99215686 0.98823529 0.58039216 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.98823529 0.90196078 0.09803922 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.02745098 0.52941176 0.99215686 0.72941176 0.04705882 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.98823529 0.8745098  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.02745098\n",
      "  0.51372549 0.98823529 0.88235294 0.27843137 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.98823529 0.56862745 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.18823529 0.64705882\n",
      "  0.98823529 0.67843137 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.3372549  0.99215686 0.88235294 0.         0.         0.\n",
      "  0.         0.         0.         0.44705882 0.93333333 0.99215686\n",
      "  0.63529412 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.98823529 0.97647059 0.57254902 0.18823529 0.11372549\n",
      "  0.33333333 0.69803922 0.88235294 0.99215686 0.8745098  0.65490196\n",
      "  0.21960784 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.98823529 0.98823529 0.98823529 0.89803922 0.84313725\n",
      "  0.98823529 0.98823529 0.98823529 0.76862745 0.50980392 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.10980392 0.78039216 0.98823529 0.98823529 0.99215686 0.98823529\n",
      "  0.98823529 0.91372549 0.56862745 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.09803922 0.50196078 0.98823529 0.99215686 0.98823529\n",
      "  0.55294118 0.14509804 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n",
      "WEIGHS\n",
      "[[0.3 0.7]\n",
      " [0.2 0.6]\n",
      " [0.4 0.2]\n",
      " [0.5 0.5]]\n",
      "LAYER\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.21568627\n",
      "  0.53333333 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.6745098\n",
      "  0.99215686 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.07058824 0.88627451\n",
      "  0.99215686 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.19215686 0.07058824 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.67058824 0.99215686\n",
      "  0.99215686 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.11764706 0.93333333 0.85882353 0.31372549 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.09019608 0.85882353 0.99215686\n",
      "  0.83137255 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.14117647 0.99215686 0.99215686 0.61176471 0.05490196 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.25882353 0.99215686 0.99215686\n",
      "  0.52941176 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.36862745 0.99215686 0.99215686 0.41960784 0.00392157 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.09411765 0.83529412 0.99215686 0.99215686\n",
      "  0.51764706 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.60392157 0.99215686 0.99215686 0.99215686 0.60392157 0.54509804\n",
      "  0.04313725 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.44705882 0.99215686 0.99215686 0.95686275\n",
      "  0.0627451  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.01176471\n",
      "  0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      "  0.74509804 0.1372549  0.         0.         0.         0.\n",
      "  0.         0.15294118 0.86666667 0.99215686 0.99215686 0.52156863\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.07058824\n",
      "  0.99215686 0.99215686 0.99215686 0.80392157 0.35294118 0.74509804\n",
      "  0.99215686 0.94509804 0.31764706 0.         0.         0.\n",
      "  0.         0.58039216 0.99215686 0.99215686 0.76470588 0.04313725\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.07058824\n",
      "  0.99215686 0.99215686 0.77647059 0.04313725 0.         0.00784314\n",
      "  0.2745098  0.88235294 0.94117647 0.17647059 0.         0.\n",
      "  0.18039216 0.89803922 0.99215686 0.99215686 0.31372549 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.07058824\n",
      "  0.99215686 0.99215686 0.71372549 0.         0.         0.\n",
      "  0.         0.62745098 0.99215686 0.72941176 0.0627451  0.\n",
      "  0.50980392 0.99215686 0.99215686 0.77647059 0.03529412 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.49411765\n",
      "  0.99215686 0.99215686 0.96862745 0.16862745 0.         0.\n",
      "  0.         0.42352941 0.99215686 0.99215686 0.36470588 0.\n",
      "  0.71764706 0.99215686 0.99215686 0.31764706 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.53333333\n",
      "  0.99215686 0.98431373 0.94509804 0.60392157 0.         0.\n",
      "  0.         0.00392157 0.46666667 0.99215686 0.98823529 0.97647059\n",
      "  0.99215686 0.99215686 0.78823529 0.00784314 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.68627451\n",
      "  0.88235294 0.36470588 0.         0.         0.         0.\n",
      "  0.         0.         0.09803922 0.58823529 0.99215686 0.99215686\n",
      "  0.99215686 0.98039216 0.30588235 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.10196078\n",
      "  0.6745098  0.32156863 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.10588235 0.73333333 0.97647059\n",
      "  0.81176471 0.71372549 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.65098039\n",
      "  0.99215686 0.32156863 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.25098039\n",
      "  0.00784314 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.\n",
      "  0.94901961 0.21960784 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.96862745\n",
      "  0.76470588 0.15294118 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.49803922\n",
      "  0.25098039 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n",
      "MATMUL\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 28 is different from 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[141], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m nn \u001b[38;5;241m=\u001b[39m NeuralNet((train_X, train_y))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# batches = nn.create_batches()\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m batches \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# print(batches[0])\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[139], line 119\u001b[0m, in \u001b[0;36mNeuralNet.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m--> 119\u001b[0m         epoch_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m / \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mepoch_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[139], line 133\u001b[0m, in \u001b[0;36mNeuralNet.train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m#         for batch in batches:\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m#             print(\"BATCH\")\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m#             print(batch)\u001b[39;00m\n\u001b[1;32m    132\u001b[0m         batch \u001b[38;5;241m=\u001b[39m batches[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 133\u001b[0m         epoch_error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;66;03m# Adding epoch cost to the list containing all losses across all epochs\u001b[39;00m\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverall_error\u001b[38;5;241m.\u001b[39mappend(epoch_error)\n",
      "Cell \u001b[0;32mIn[139], line 152\u001b[0m, in \u001b[0;36mNeuralNet.train_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;66;03m# Outputs in batch\u001b[39;00m\n\u001b[1;32m    151\u001b[0m         o \u001b[38;5;241m=\u001b[39m outputs\n\u001b[0;32m--> 152\u001b[0m         feedforward_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeedforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m#         net_output = self.net_output(o)\u001b[39;00m\n\u001b[1;32m    154\u001b[0m         \n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m#         batch_output_error += self.example_error(feedforward_output, net_output)\u001b[39;00m\n\u001b[1;32m    156\u001b[0m         batch_output_error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexample_error(feedforward_output, o)\n",
      "Cell \u001b[0;32mIn[139], line 94\u001b[0m, in \u001b[0;36mNeuralNet.feedforward\u001b[0;34m(self, input_layer)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(current_layer)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMATMUL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweighs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_layer\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Saving vectors of inputs to consecutive net layers, that will later\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# be used during the error backpropagation algorithm\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINFEEDFORWARD\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 28 is different from 2)"
     ]
    }
   ],
   "source": [
    "nn = NeuralNet((train_X, train_y))\n",
    "# batches = nn.create_batches()\n",
    "batches = nn.train()\n",
    "# print(batches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f3fd30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
